# Web Scraper Project

This project is a web scraper designed to crawl specified URLs and extract content from web pages. The crawler will start from a list of priority URLs and will log its activities, including any errors encountered during the crawling process.

## Project Structure

The project has the following structure:

```
scraper/
├── seeds.txt          # Priority URLs to crawl
├── crawler.py         # Main crawler script
├── crawlLog.txt       # Detailed logging output
└── output/
    ├── MDs/          # Markdown files for each page
    │   └── Home.md   # Example crawled page
    ├── index.jsonl    # JSON index of all crawled pages
    └── failed_urls.txt # URLs that failed to crawl
```

## Files Description

- **seeds.txt**: This file contains the priority URLs that the crawler will use as starting points for crawling. You can add or modify URLs as needed.

- **crawler.py**: This is the main script for the web crawler. It contains the logic for crawling the web pages, extracting content, and saving the output.

- **crawlLog.txt**: This file stores detailed logging output from the crawler, including information about the crawling process and any errors encountered. It is useful for debugging and monitoring the crawler's performance.

- **output/**: This directory holds all the output generated by the crawler.

  - **MDs/**: This subdirectory contains Markdown files for each crawled page. Each file represents the content extracted from a specific page.

  - **Home.md**: This file serves as an example of a crawled page in Markdown format.

  - **index.jsonl**: This file is a JSON Lines file that serves as an index of all crawled pages, providing a structured format for easy access to the crawled data.

  - **failed_urls.txt**: This file lists URLs that failed to crawl, allowing you to review and troubleshoot any issues with specific links.
